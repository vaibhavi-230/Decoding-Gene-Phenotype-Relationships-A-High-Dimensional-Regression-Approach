# -*- coding: utf-8 -*-
"""J and J.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t4blNh2khvBrS-d4J6NkPILCw27WFhBL

# Data
"""

import pandas as pd

gene_df = pd.read_csv('gene.csv')
phenotype_df = pd.read_csv('phenotype.csv')

"""## gene.csv"""

gene_df.head()

gene_df.info()

gene_df.describe()

gene_df.isnull().sum().sum()

gene_df.boxplot()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12, 6))
sns.boxplot(data=gene_df.iloc[:, :100])  # Assuming first 10 columns are gene expressions
plt.title("Boxplots for first 100 Gene Expressions")
plt.xlabel("Genes")
plt.ylabel("Expression Levels")
plt.xticks(rotation=45)
plt.show()

import pandas as pd
import numpy as np

# Function to calculate outliers for each gene
def find_outliers_iqr(gene_df):
    outlier_rows = []
    for gene in gene_df.columns:
        Q1 = gene_df[gene].quantile(0.25)
        Q3 = gene_df[gene].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        # Find rows where this gene has outliers
        outliers = gene_df[(gene_df[gene] < lower_bound) | (gene_df[gene] > upper_bound)]
        outlier_rows.append(outliers.index.tolist())
    return outlier_rows

# Get outliers for all genes
outlier_rows = find_outliers_iqr(gene_df)

# Flatten the list of outlier rows
flattened_outliers = [item for sublist in outlier_rows for item in sublist]

# Count how many times each sample (row) is flagged as an outlier
outlier_counts = pd.Series(flattened_outliers).value_counts()
# Convert the Series to a DataFrame and rename columns
outlier_df = outlier_counts.reset_index()
outlier_df.columns = ['Row No.', 'No. of Outliers']

# Print the DataFrame with the proper column headings
print(outlier_df)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12, 10))
sns.heatmap(gene_df.iloc[:, :100].corr(), annot=False, cmap='coolwarm') # correlation for first 100 variables
plt.title("Correlation Heatmap of first 100 Genes")
plt.show()

# pCorrelation heatmap for the first 10 independent variables

import matplotlib.pyplot as plt
plt.figure(figsize=(12, 10))
sns.heatmap(gene_df.iloc[:, :10].corr(), annot=True, cmap='coolwarm') # correlation for first 10 variables
plt.title("Correlation Heatmap of first 10 Genes")
plt.show()

import matplotlib.pyplot as plt

# Hisogram of some independent variables

plt.figure(figsize=(12, 8))

plt.subplot(2, 2, 1)
plt.hist(gene_df['X1'], bins=20)  # Adjust 'bins' as needed
plt.title('Histogram of X1')
plt.xlabel('X1 Values')
plt.ylabel('Frequency')

plt.subplot(2, 2, 2)
plt.hist(gene_df['X2'], bins=20)
plt.title('Histogram of X2')
plt.xlabel('X2 Values')
plt.ylabel('Frequency')

plt.subplot(2, 2, 3)
plt.hist(gene_df['X3'], bins=20)
plt.title('Histogram of X3')
plt.xlabel('X3 Values')
plt.ylabel('Frequency')

plt.subplot(2, 2, 4)
plt.hist(gene_df['X4'], bins=20)
plt.title('Histogram of X4')
plt.xlabel('X4 Values')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

# Calculate skewness for each independent variable
skewness_values = gene_df.iloc[:, 0:].skew()  # Exclude the first column (assuming it's an index or ID)

# Count positively and negatively skewed variables
positive_skew_count = (skewness_values > 0).sum()
negative_skew_count = (skewness_values < 0).sum()

print(f"Number of positively skewed independent variables: {positive_skew_count}")
print(f"Number of negatively skewed independent variables: {negative_skew_count}")

"""## phenotype.csv"""

phenotype_df.head()

import matplotlib.pyplot as plt
plt.figure(figsize=(10, 6))
sns.histplot(phenotype_df['V1'], kde=True)
plt.title('Histogram of Phenotype with KDE')
plt.xlabel('Phenotype Value')
plt.ylabel('Frequency')
plt.show()

"""This shows that phenotype values are approximately centered about zero."""

# Boxplot for phenotype
plt.figure(figsize=(8, 6))
sns.boxplot(y=phenotype_df['V1'])
plt.title('Boxplot of Phenotype')
plt.ylabel('Phenotype Value')
plt.show()

import scipy.stats as stats

# Q-Q plot for phenotype
plt.figure(figsize=(8, 6))
stats.probplot(phenotype_df['V1'], dist='norm', plot=plt)
plt.title('Q-Q Plot of Phenotype')
plt.show()

# Shapiro-Wilk test for normality
shapiro_test = stats.shapiro(phenotype_df['V1'])
print("Shapiro-Wilk Test:")
print(f"Statistic: {shapiro_test.statistic}")
print(f"P-value: {shapiro_test.pvalue}")

alpha = 0.05
if shapiro_test.pvalue > alpha:
    print("Sample looks Gaussian (fail to reject H0)")
else:
    print("Sample does not look Gaussian (reject H0)")

"""# Screening method - Correlation-based"""

# Perform correlation analysis using absolute correlations
correlations = gene_df.corrwith(phenotype_df['V1'])

# Filter genes based on correlation threshold (e.g., absolute correlation > 0.2)
correlation_threshold = 0.2
significant_genes = correlations[abs(correlations) > correlation_threshold]

print("Significant genes based on correlation with phenotype 'V1':")
significant_genes

selected_genes = significant_genes.index.tolist()

# Create a new DataFrame with only the selected genes
selected_genes_df = gene_df[selected_genes]

# Print the first few rows of the new DataFrame
print(selected_genes_df.head())

selected_genes_df.shape

"""# High dimensional Regression Approach

## Ridge
"""

import pandas as pd
from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Prepare the data
X = selected_genes_df  # Use the filtered genes as features
y = phenotype_df['V1']  # Use 'V1' as the target variable

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Example test size and random state

# Initialize and train the Ridge regression model
ridge_model = Ridge(alpha=1.0) # You can tune the alpha parameter
ridge_model.fit(X_train, y_train)

# Get coefficients (weights) for each gene
coefficients = ridge_model.coef_

# Create a DataFrame for coefficients and gene names
coefficients_df = pd.DataFrame({'Gene': selected_genes, 'Coefficient': coefficients})

# Sort the DataFrame by the absolute value of the coefficients to get the most significant genes
coefficients_df['Abs_Coefficient'] = abs(coefficients_df['Coefficient'])
top_genes = coefficients_df.sort_values(by='Abs_Coefficient', ascending=False).head(10)

# Print the top 10 most significant genes
print("Top 10 most significant genes (Ridge Regression):")
print(top_genes)


# Make predictions on the test set
y_pred = ridge_model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")
print('\n')

print("Intercept:", ridge_model.intercept_)
print('\n')

# Evaluate the model
print("R-squared:", ridge_model.score(X, y))

import numpy as np
residuals = y_test - y_pred
std_dev_residuals = np.std(residuals)
print(f"Standard Deviation of Residuals: {std_dev_residuals}")

"""## Lasso"""

import pandas as pd
from sklearn.linear_model import Lasso

# Prepare the data
X = selected_genes_df
y = phenotype_df['V1']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Lasso Regression model
lasso_model = Lasso(alpha=0.1) # You might need to tune alpha
lasso_model.fit(X_train, y_train)

# Get the coefficients from the lasso regression model
coefficients = lasso_model.coef_

# Create a DataFrame to store the coefficients and gene names
coef_df = pd.DataFrame({'Gene': selected_genes, 'Coefficient': coefficients})

# Sort the DataFrame by the absolute value of the coefficients in descending order
coef_df['Abs_Coefficient'] = abs(coef_df['Coefficient'])
coef_df = coef_df.sort_values('Abs_Coefficient', ascending=False)

# Get the top 10 most significant genes
top_10_genes = coef_df.head(10)

# Print the top 10 genes and their coefficients
print(top_10_genes[['Gene', 'Coefficient']])

from sklearn.metrics import r2_score

# Make predictions on the test set
y_pred = lasso_model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

import numpy as np
# Calculate residuals
residuals = y_test - y_pred

# Calculate the standard deviation of residuals
residual_std_dev = np.std(residuals)

print(f"Standard Deviation of Residuals: {residual_std_dev}")

"""## Elastic Net"""

import pandas as pd
import numpy as np
from sklearn.linear_model import ElasticNet
from sklearn.metrics import mean_squared_error, r2_score

# Prepare the data
X = selected_genes_df
y = phenotype_df['V1']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the ElasticNet model
elastic_net_model = ElasticNet(alpha=0.1, l1_ratio=0.5) # You might need to tune alpha and l1_ratio
elastic_net_model.fit(X_train, y_train)

# Get the coefficients from the elastic net model
coefficients = elastic_net_model.coef_

# Create a DataFrame to store the coefficients and gene names
coef_df = pd.DataFrame({'Gene': selected_genes, 'Coefficient': coefficients})

# Sort the DataFrame by the absolute value of the coefficients in descending order
coef_df['Abs_Coefficient'] = abs(coef_df['Coefficient'])
coef_df = coef_df.sort_values('Abs_Coefficient', ascending=False)

# Get the top 10 most significant genes
top_10_genes = coef_df.head(10)

# Print the top 10 genes and their coefficients
print(top_10_genes[['Gene', 'Coefficient']])

# Make predictions on the test set
y_pred = elastic_net_model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

# Calculate residuals
residuals = y_test - y_pred

# Calculate the standard deviation of residuals
residual_std_dev = np.std(residuals)

print(f"Standard Deviation of Residuals: {residual_std_dev}")

"""## Bayesian Ridge"""

import pandas as pd
import numpy as np
from sklearn.linear_model import BayesianRidge


# Prepare the data
X = selected_genes_df
y = phenotype_df['V1']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Bayesian Ridge Regression model
bayesian_ridge_model = BayesianRidge()
bayesian_ridge_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = bayesian_ridge_model.predict(X_test)


# Get coefficients (weights) for each gene
coefficients = bayesian_ridge_model.coef_

# Create a DataFrame for coefficients and gene names
coefficients_df = pd.DataFrame({'Gene': selected_genes, 'Coefficient': coefficients})

# Sort the DataFrame by the absolute value of the coefficients to get the most significant genes
coefficients_df['Abs_Coefficient'] = abs(coefficients_df['Coefficient'])
top_genes = coefficients_df.sort_values(by='Abs_Coefficient', ascending=False).head(10)

# Print the top 10 most significant genes
print("Top 10 most significant genes (Bayesian Ridge Regression):")
print(top_genes)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

# Calculate residuals
residuals = y_test - y_pred

# Calculate the standard deviation of residuals
residual_std_dev = np.std(residuals)

print(f"Standard Deviation of Residuals: {residual_std_dev}")

"""## Bayesian Lasso"""

import pandas as pd
import numpy as np
from sklearn.linear_model import BayesianRidge
from sklearn.linear_model import ARDRegression


# Prepare the data
X = selected_genes_df
y = phenotype_df['V1']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Bayesian ARD Regression model (a form of Bayesian Lasso)
bayesian_lasso_model = ARDRegression()
bayesian_lasso_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = bayesian_lasso_model.predict(X_test)

# Get coefficients (weights) for each gene
coefficients = bayesian_lasso_model.coef_

# Create a DataFrame for coefficients and gene names
coefficients_df = pd.DataFrame({'Gene': selected_genes, 'Coefficient': coefficients})

# Sort the DataFrame by the absolute value of the coefficients to get the most significant genes
coefficients_df['Abs_Coefficient'] = abs(coefficients_df['Coefficient'])
top_genes = coefficients_df.sort_values(by='Abs_Coefficient', ascending=False).head(10)

# Print the top 10 most significant genes
print("Top 10 most significant genes (Bayesian ARD Regression - Bayesian Lasso):")
print(top_genes)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

# Calculate residuals
residuals = y_test - y_pred

# Calculate the standard deviation of residuals
residual_std_dev = np.std(residuals)

print(f"Standard Deviation of Residuals: {residual_std_dev}")